\documentclass[]{report}   % list options between brackets
\usepackage{xcolor}
\usepackage{listings}
\lstset
{ %Formatting for code in appendix
    %basicstyle=\footnotesize,
    % line number stuff
    numbers=left,
    stepnumber=1,
    xleftmargin=4.0ex,
    showstringspaces=false,
    tabsize=4,
    breaklines=true,
    breakatwhitespace=false,
}

\usepackage{textcomp}
\lstset{basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{red},
  keywordstyle=\color{blue}
}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
 
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
 
\usepackage{courier}
% type user-defined commands here


\definecolor{titlebgdark}{RGB}{0,163,243}
\definecolor{titlebglight}{RGB}{191,233,251}

%\usepackage{mystyle} % Nannas hacks
\usepackage{enumitem}
\begin{document}
\sloppy

					
\lstset{language=Python,upquote=true}
\setlist[enumerate]{itemsep=0mm}
\setlist[itemize]{itemsep=0mm}
\setlength\parindent{0pt}

\title{WeBook: scrape Webpage to Ebook}
\author{Jan Christian Refsgaard}
\date{December 20, 2017}    % type date between braces
\maketitle

%-------------------------------------------------------------------------------
% ABBREVIATIONS
%-------------------------------------------------------------------------------
\section*{Abbreviations}
\begin{tabular}{ l l }
	% TODO: remember to sort them !!!!
	% TODO: remember to sort them !!!!
	\textbf{URI}   & \textbf{U}niform \textbf{R}esource \textbf{I}dentifier \\
	\textbf{URL}   & \textbf{U}niform \textbf{R}esource \textbf{L}ocator \\
	\textbf{EPUB}  & \textbf{E}lectronic \textbf{PUB}lication  \\
	\textbf{HTML}  & \textbf{H}yper\textbf{T}ext \textbf{M}arkup \textbf{l}anguage \\
	\textbf{XHTML} & \textbf{E}xtensible \textbf{H}yper\textbf{T}ext 
					 \textbf{M}arkup \textbf{l}anguage \\
	\textbf{HTML5} & \textbf{H}yper\textbf{T}ext \textbf{M}arkup \textbf{l}anguage 
					 version 5 \\
	\textbf{OEBPS} & \textbf{O}pen \textbf{EB}ook \textbf{P}ublication \textbf{S}tructure \\
\end{tabular}

%-------------------------------------------------------------------------------
% INTRODUCTION
%-------------------------------------------------------------------------------
\chapter{Introduction}           
Many aspiring authors publish their work on as a wordpress wordpress blog, two
of my favorite books are only available this way:
\begin{itemize}
    \item Worm: \url{parahumans.wordpress.com/}.
    \item From The New World: \url{shinsekai.cadet-nine.org/} 
\end{itemize}
There are other webpages such as \url{fanfiction.net} where users have uploaded millions of books.

Because the above books are only available as websites they cannot be consumed
from an e-book reader. Nor can they be enjoyed when internet is unavailable such
as on an air plane.

\section{Problem Statement}
How can a website be converted to an e-book?

This question can be further broken down to:
\begin{itemize}
    \item how do you scrape a website
    \begin{itemize}
        \item do you have to write a scraper for each website, or can the software be made more generic?
    \end{itemize}
    \item which e-book format resemble a website most?
    \item should the software support multiple e-book formats?
    \item How do we make the tool user friendly.
\end{itemize}

\subsection{Scope}
Many e-book format exists, but EPUP is based on html, and therefore the easiest
to support when the source book is also HTML. I will not support other formats
as Calibre \url{https://calibre-ebook.com/}, support conversion between most
e-book formats.

As there are a plethora of different webpage books, that all will be needed to
be converted to EPUP, I will only write a scraper for \url{fanfiction.net}, but
strive to make the code modular so it is easy for tech savvy users to write a
parser, to further facilitate the expansion of the tool, the code is available
on github under \url{https://github.com/jancr/webook} under the MIT licence. 

To make the tool available I will make a website, as well as a command-line
interface.

My Hope is for my tool to be used by other readers and programmers. Thus to
make installation as easy as possible I will strive to mostly use functions
from Pythons standard library.

%\subsection{Methods}

%-------------------------------------------------------------------------------
% THEORY
%-------------------------------------------------------------------------------
\chapter{Theory}
\section{Electronic Publication (EPUB)}
\subsection{EPUB History}
EPUB is the successor of the Open eBook Publication Structure (OEBPS) version
1.2. For this reason the first version of ePub, version 2.0, was released in
2007. In 2010 it reached maturity and the final specification EPUB 2.0.1 was
releasesd \ref{epub201}. This promptet the development of version 3 where there
is still active development\ref{epub301}. The Difference between the 2.0 and
3.0 mostly new features such as support for mathML and support for Fixed Layout Documents such as comments\ref{epub2to3}. Also version 2.0 uses XHTML where version 3.0 supports both XHTML and HTML5.EPUB 3.0.1 is mostly backwards compatible with 2.0.1\ref{epub2to3}. 

\subsection{Implementation}
As many e-eook readers have different screen sizes, the standard has opted to
use HTML + css, to facilitate reflowable documents.

An EPUB is basically a website, contained in a ZIP archive. To see this lets download a public domain book (note I am using the back tick hack to split the url over multiple lines):

\begin{lstlisting}[language=bash]
wget "https://s3-us-west-2.amazonaws.com/"`
     `"pressbooks-samplefiles/MetamorphosisJacksonTheme/"`
     `"Metamorphosis-jackson.epub"
unzip Metamorphosis-jackson.epub
\end{lstlisting}

Which yields the following output:

\begin{lstlisting}[language=bash]
Archive:  Metamorphosis-jackson.epub
 extracting: mimetype
  inflating: toc.ncx
  inflating: OEBPS/chapter-001-chapter-i.html
            ...
  inflating: OEBPS/jackson.css
  inflating: book.opf
  inflating: META-INF/container.xml
  inflating: META-INF/com.apple.ibooks.display-options.xml
\end{lstlisting}
% removed from ...
  % inflating: OEBPS/chapter-002-chapter-ii.html
  % inflating: OEBPS/title-page.html
  % inflating: OEBPS/front-cover.html
  % inflating: OEBPS/chapter-003-chapter-iii.html
  % inflating: OEBPS/copyright.html
  % inflating: OEBPS/table-of-contents.html
  % inflating: OEBPS/assets/pressbooks-promo.png
  % inflating: OEBPS/assets/MedulaOne-Regular.ttf
  % inflating: OEBPS/assets/themetamorphosis_1200x1600.jpg
  % inflating: OEBPS/pressbooks-promo.html

We see above that the website aspects of the e-book is located in the folder
\texttt{OEBPS}, while this is not mandatory, it is common practice, the folder
is named \texttt{OEBPS} because that is the predecessor to EPUB.

Apart from the website aspects of \texttt{Metamorphosis-jackson.epub} there
three other files mandated by the EPUB standard\ref{epub301}:
\begin{itemize}
    \item \texttt{META-INF/container.xml} This XML file points to the file OPF file(s)
    \item \texttt{book.opf}: Open Package Format (URI:
          \url{http://www.idpf.org/2007/opf}), a file containing meta data, such as
          author name, and a manifest indexing the books content, in this case all
          the files in OEBPS)
    \item \texttt{toc.ncx} The table of context, an XML file (URI:
          \url{http://www.daisy.org/z3986/2005/ncx/}.
\end{itemize}

\section{Python 3}
\subsection{Quick Python Intro for Programmers}
Python is one of the easiest languages to learn. The standard hello world is only one line:
\begin{lstlisting}[language=python]
print("hello world")
\end{lstlisting}

Python uses whitespace to signify end of code block, thus all code that have the same indentation are in the same block. This avoids the dangling else problem\ref{dangling_else}:

\begin{lstlisting}[language=python]
y = x = 2
if y == 3:
    if x == 3:
        print('x is 3')
else:  # the else belongs to the if with the same indentation
    print('y is not 3')
\end{lstlisting}

% \begin{lstlisting}[language=c]
% int y = 2
% int y = 2
% if (y == 3)
    % if (x == 3)
        % printf('yay')
% else 
    % printf('does this else belong to the y or x if statement??')
% \end{lstlisting}
% 
Python 3 is under rapid development, and in version 3.6 f strings were
introduced, allowing you to seamlessly intermingle code and strings, by
prepending stings with an \textbf{\texttt{f}}, you can run arbitrary python
code between \{ and \}:

\begin{lstlisting}[language=python]
x = 'hat,cat'
print(my f"{x.split(',')[0]} is beautiful")
# prints "my hat is beautifull"
\end{lstlisting}

For a more in depth tutorial the tutorial at \url{
https://docs.python.org/3/tutorial/index.html} is excellent.

%\subsection{OOP}
\subsection{Web Scraping}
Web scraping referes to the act of extracting data from a website, it can be
done manually, but most often it is done programmically. As crawling a server
risks overloading it, many pages have a file named
\texttt{robots.txt}\ref{robots} that specifies, which parts of the site robots
are allowed to scrape. Of course malicious bots does not have to respect the
\texttt{robots.txt}.

There are many different tools and libraries that can assist one. The most
simple way to scrape a website is simply to make GET requests to web pages,
find links, and then make GET requests to those. While this works wonders for
static pages, it falls short on dynamic webpages. For very advanced web
scraping libraries like Java's HtmlUnit, "a GUI less browser for Java
Programs"\ref{java_htmlunit}" be used to scrape dynamic pages.

Luckely \url{fanfiction.net} and similiar pages are not very dynamic, making
the request functions from the python standard library sufficient.

\subsection{Flask}
Flask is a microframework, it is thus very lightweight and ideal for small
webpages. It builds upon two other python packages. 1. Werkzeug, a WSGI (Web
Server Gateway Interface). 2) Jinja 2, a templating language that allows you to
mix python and html, much like php. 3) 

Much like python, hello world in Flask is also surprisingly few lines:

\begin{lstlisting}[language=python]
import flask
app = flask.Flask(__name__)

@app.route("/")
def hello():
    return "Hello World!"
\end{lstlisting}

\textbf{Line 1-2:} Imports Flask, and instanciates the WSGI server \\
\textbf{Line 4:} this decorator assigns the '/' request (both GET and POST) to
the function \texttt{hello} \\
\textbf{Line 5-6:} a Python functio that returns "HTTP/1.1 200 OK" with "Hello
World" as data \\

If instead of returning plain text we want to return html, then we need two
things, 1. we need the function \texttt{flask.render\_template}, that takes a Jinja 2 template as argument and processes it into html, 2. A Jinja 2 template

\subsubsection{Jinja 2}
The base idea of Jinja is that you create one base template, and extend it,
data is passed to the template via \texttt{render\_template}. 


\begin{lstlisting}[language=python]
{% extends "layout.html" %}
{% block body %}
  <ul>
  {% for user in users %}
    <li><a href="{{ user.url }}">{{ user.username }}</a></li>
  {% endfor %}
  </ul>
{% endblock %}
\end{lstlisting}

\textbf{Line 1:} This an extension of the template \texttt{layout.html} \\
\textbf{Line 2 and 8:} The following code goes into the 'body' block of
\texttt{layout.html} \\
\textbf{Line 4-6:} python code is encapsulated in \texttt{\{\% \%\}} and python
variables in \texttt{\{\{ \}\}}

Assuming the above template is called \texttt{users.html} and you want to pass
it the \texttt{user} variable, then render\_template would be called with two
arguments, 1. The template (\texttt{users.html}, and 2. The user
variable(\texttt{user})

\begin{lstlisting}[language=python]
@app.route("/users.html")
def users():
    return flask.render_templates('users.html', user=user)
\end{lstlisting}


\subsection{Concurrent Programming and parallelism}
Concurrent programming and parallelism are two different strategies for
speeding up code. An example of parallelism is multi threading or
multiprocessing where a subset of the problem is handled to each given to a
worker. If however the majority of time is spend doing computations but by
waiting on blocking calls such as waiting for a HTTP request or IO on the file
system, a substantial speed up can be achieved by scheduling other parts of the
program to run while waiting.

In Python multi threading will speed up blocking programs, but it will not
speed up non blocking programs as only one thread can be scheduled to run
inside the interpreter. In python 3.5 a new syntax (\texttt{asyng/await})
became available for concurrent programming without threads, this is a very
interesting development for the language, but as of writing this is only
supported for HTTPS/DNS using external libraries such as \texttt{aiohttp}. 


%-------------------------------------------------------------------------------
% WEBOOK 
%-------------------------------------------------------------------------------
\chapter{WeBook}

\section{Design and Technology Choices}
\subsubsection{E-book format}
Because of EPUB 3.0.1 is mostly backwards compatible with EPUB
2.0.1\ref{epub2to3} and because the EPUB 2.0.1 specification is simpler, I have
opted for creating EPUB that conform to both standards. This is achieved by
using XHTML instead of HTML5.

\subsubsection{Python and Libraries}
To keep the project simple, I have opted to use the same language (Python) for:
scraping the webpages, creating the e-book, exposing the command line
interface and creating web server.
\textbf{E-Book generation:} As the EPUB is a mixture of xhtml and xml, a xml
parsing library is needed. The two best candidates are \texttt{ElementTree}
from the standard library or the extenal library \texttt{lxml}. They both have
very similiar syntax, and \texttt{ElementTree} is basically a subset of
\texttt{lxml}. I don't stricktly need any of the features from \texttt{lxml}
though it's handling of name spaces is better than \texttt{ElementTree}, so to
reduce the number of dependencies I have opted for \texttt{ElementTree}

\textbf{scraping}. There are many good python libraries that can simplify the for web-scraping.
\begin{itemize}
    \item \textbf{\texttt{mechanize}:} Which has a very natural and powerful API
    \item \textbf{\texttt{urllib}:} Part of the Python standard library, can make GET requests.
    \item \textbf{\texttt{requests}:} One of the most popular Python libraries,
        has better API than urllib, but is not part of the standard library
    \item \textbf{\texttt{Beautiful Soup}:} Prominent html Parsing library
\end{itemize}
As I have a commitment to keeping the number of dependencies low I will parse
urls and make requests using \texttt{urllib} from the standard library. But as
many web pages can be malformed I need a mature library, that parses html much
the same way as the browser, and have thus settled on \texttt{Beautiful Soup}.

\section{Code and File Structure}
%\subsection{EPUB Files}
The base idea is that for each book type (EPUB, MOBI etc), the base skeleton will be available
under \texttt{book\_templates}, for now only EPUB is available (\texttt{book\_templates\/epub\/}).

% \begin{lstlisting}[language=bash,basicstyle=\small]
% $ ls -lh book_templates/epub/*
% -rwxr-xr-x  1 jcr  staff   934B Nov 22 12:23 content.opf
% -rwxr-xr-x@ 1 jcr  staff    20B Jun  4 23:19 mimetype
% -rwxr-xr-x@ 1 jcr  staff    50B Jun  4 23:19 page_style.css
% -rwxr-xr-x@ 1 jcr  staff   404B Jun  4 23:19 page_template.xhtml
% -rwxr-xr-x@ 1 jcr  staff   670B Jun  4 23:19 titlepage.xhtml
% -rwxr-xr-x  1 jcr  staff   578B Nov 22 17:40 toc.ncx
% 
% META-INF:
% -rwxr-xr-x@ 1 jcr  staff   225B Nov 22 19:06 container.xml
% \end{lstlisting}
% 
\subsection{Static Files}
Here I will describe all the files that are hard coded, and thus not changed by the code. 

\texttt{mimetype} specifies the EPUP mimetype (\texttt{application/epub+zip}).
\texttt{META-INF/container.xml} points out where the root file is located, it
is hard coded to point to \texttt{content.opf}. \texttt{page\_style.css}
contains the css to style the book. While I could allow the parser to also
scrape style sheets, I think it is better to hard code it as some pages may
have the style sheet cascade in such a way that they are not applied correctly,
when only a small portion of the web page is scraped.

\subsubsection{XHTML Files}

\subsubsection{content.opf}
The \texttt{content.opf} is an xml file with two important tags
\begin{itemize}
    \item \texttt{<metadata>}, such as cover image, author name and book title.
    \item \texttt{<manifest>}, a reference to every file that needs to be
        indexed, it is hard coded to have references to \texttt{titlepage.xhtml}, \texttt{page\_style.css} and \texttt{toc.ncx}. Each chapter scraped should result in an xhtml file with an entry here.
\end{itemize}

\subsubsection{toc.ncx}
Contains the Table of Content. This file contains two important tags: 
1. \texttt{<docTitle>}: where you put the title of the book
2. \texttt{<navMap>}, an xml structure that specifies the nesting of book sections and "read order" (\texttt{playOrder}). Here is the xml my tool generated for a simple 1 page story:

\begin{lstlisting}[language=XML]
<navMap>
    <navPoint id="navPoint-1" playOrder="1">
        <navLabel><text>Etcetera</text></navLabel>
        <content src="titlepage.xhtml"/>
    </navPoint>
    <navPoint id="navPoint-2" playOrder="2">
        <navLabel><text>Etcetera</text></navLabel>
        <content src="short_story.xhtml"/>
    </navPoint>
  </navMap>
\end{lstlisting}

The \texttt{navPoints} can be nested, resulting in 'subsections'.

\subsection{EBook}
This class (located in \texttt{webook/webook.py} contains all the logic needed
to create an Ebook. It has one very important abstract method \texttt{scrape}
that subclasses should implement scrape webpages.

\subsubsection{High Level Overview}
The EBook class does the following:

\begin{itemize}
	\item moves all files from \texttt{book\_templates/epub} to a temporary directory
	\item creates datastructures to manipulate the \texttt{content.opf} and \texttt{toc.ncx} xml files
	\item calls the abstract method crawl, 
	\begin{itemize}
		\item  This class should crawl the target site, and (if possible) set
			\texttt{self.title}, \texttt{self.first\_name} and \texttt{self.last\_name} (of the author), and
			\texttt{self.cover\_path} (to add a cover image).
		\item for each book chapter/section/arch etc. created
			\texttt{self.write\_html} needs to be called to write the chapter
			to file, and \texttt{self.update} needs to be called to update the
			\texttt{content.opf} and \texttt{toc.ncx} data structures.
	\end{itemize}
		\item dumps the updated \texttt{content.opf} and \texttt{toc.ncx} to disk. 
		\item create a zip archive of the temporary directory and give it the \texttt{.epub} extension
		\item delete the temporary directory
\end{itemize}

\subsubsection{Documenting Key Methods}
\textbf{The constructor} (\texttt{\_\_init\_\_}) takes the following arguments:
\begin{itemize}
	\item \texttt{url}: The website to scrape
	\item \texttt{epub\_file}: The filename of the ebook generated
	\item \texttt{title}: The title of the book, if not None, it overwrites the scraped book name.
	\item \texttt{workers}: The max number of concurrent TCP requests
	\item \texttt{run}: Set to false if you want to create the python object without also creating the book right away, this is necessary if you want to add a status bar. 
\end{itemize}

The constructor initiates the following important objects
\begin{itemize}
	\item \texttt{output\_dir}: the path to the temporary folder, where the book will be build.
	\item \texttt{toc}: An ElementTree (xml object), to facilitate manipulation of the \texttt{toc.ncx} (Table of Content xml file)
	\item \texttt{toc\_dict}: A dictionary mapping that maps names (usually of chapters) to their table of content xml element, 
	\item \texttt{content*} a set of instance variables for easy alteration of the \texttt{content.opf} xml file.
\end{itemize}


\textbf{run}: This function is a generator, that \texttt{yield} the progress of
the webscraping. When it is done scraping it updates the author name, title,
and cover image. This is done after scraping is complete,
because it assumes that the scraper will set these variables as it scrapes the
webpages. Finally it calls \texttt{self.save(self.epub\_file)} create the ebook.


            % self.update_author(self.first_name, self.last_name)
% title
% (\texttt{self.update\_title(self.title)}), which it assumes was scraped by
% \texttt{self.scrape}. Then it adds a cover a cover image
% (\texttt{self.update_title(self.title)}) which it again assumes was set by
% scrape. Finally it saves the book (\texttt{self.save(self.epub_file)}).

\textbf{save}: This method dumps the \texttt{toc} and \texttt{content} to disk, zip the temporary folder, and renames it to \texttt{epub\_file}, subsequently it deletes the temporary directory.

\textbf{write\_html}: This method creates an html file corresponding to a section/chapter/etc. it takes 3 arguments:
\begin{itemize}
	\item \texttt{text} a string or Beautiful Soup tag that will be written to file.
	\item \texttt{name}, this is the name of the file on disk (with no extension), but it is also the name used to referee to it's place in the TOC. 
	\item \texttt{header} (optional): The heading, eg "Chapter 2" or "The boy who lived" or similiar, if None it will use \texttt{name} as \texttt{header}
\end{itemize}

\textbf{update}: This method makes the ebook "aware" that a new book segment
have been added, by updating \texttt{content} and \texttt{toc}. In previous
iterations of the tool this method was called by \texttt{write\_html}. However
when I made the code asynchronous, such that multiple book chapters can be
crawled and written concurrently I had to make this into it's own function, as
you have to update the Table of Content in order!

\subsection{FanFictionEBook}
%TODO: this should be very short as the focus of the repport should be on the web stuff
% though it is mainly web scraping...

% While it may sound very scary to subclass EBook, the parser for parsing
% `www.fanfiction.net/` (`FanFictionEBook`) is less than 30 lines of code. To
% read the code for inspiration, see `webook/modules/fanfiction.py`


\subsection{webserver}
The webserver uses Flask as the backend, and JQuery + Bootstrap as the frontend. The expected path through the server is the following:
\begin{itemize}
	\item a request to '/' returns the index page \texttt{static/index.html}.
		Where the user enters the url a to an E-Book, and selects a parser
		(Only fanfiction.net is implemented as of writing).
	\item When Submit is clicked. Javascript opens a stream to the server,
		the "stream request" hands the url and parser to the back end.
	\item the backend starts scraping the chapters, for each chapter downloaded
		the progress is feed back to client, where it it is used to update the
		progress bar.
	\item When all chapters have been crawled The server saves the file to
		disk, and sends the name of the file name back to trough the stream.
	\item This prompts the client to close the stream, and make a new request
		\texttt{/download\_ebook/<file-name>} to download the file.
	\item The server cleans up all but the newest $processes\times2 + 5$ files.
\end{itemize}

\subsubsection{Frontend}
To make the webpage beautiful and responsive I opted for bootstrap\ref{bootstrap} with the
dark theme cyborg\ref{bootstrap_cyborg}. I wrote all the html by hand.
\texttt{WTForms}\ref{wtforms} also looked very promising, as they are Python object that
can be render into jinja templates seamlessly. They however adding another
dependency seemed like overkill considering I only have 2 data carrying form
elements!

\subsubsection{Backend}
Getting the status bar to work was challenging, because it required a large
rewrites of the Ebook class, The first iteration of the Ebook class and it's
subclass FanFictionEBook was written in such a way that they blocked the Python
interpreter until the book was created. Which made the Status bar jump from 0\% strait
to 100\%. This issue was solved by turning \texttt{Ebook.scrape}
into a generator function. This generator is returned when you call
\texttt{EBook.run()} as seen in line \ref{code:create-ebook-run}. The generator is
advanced by calling \texttt{next} on it, either explicitly as seen in
line \ref{code:create-ebook-next}, or implicitly by looping over it as in line
\ref{code:create-ebook-for}.

\begin{lstlisting}[language=python, escapeinside={(*}{*)}, basicstyle=\small]
@app.route('/create_ebook/<parser>/<url>')
def create_ebook(parser, url):
    def generate(ebook_parser, tmp_file):
		ebook_generator = ebook_parser.run() (*\label{code:create-ebook-run}*)
		progress = next(ebook_generator) (*\label{code:create-ebook-next}*)
        total = ebook_parser.total
        yield f"data: {round(100 * progress / total)}\n\n"
		for progress in ebook_generator: (*\label{code:create-ebook-for}*)
            yield f"data: {round(100 * progress / total)}\n\n"
        tmp_file = os.path.basename(tmp_file)
        yield f"data: file-name: {tmp_file}\n\n"
	...
    return Response(generate(ebook_parser, tmp_file), mimetype='text/event-stream')
\end{lstlisting}


% \subsection{webscraping}
% \subsection{modules}

%-------------------------------------------------------------------------------
% CONCLUSION
%-------------------------------------------------------------------------------
\chapter{Conclusion}


\begin{thebibliography}{9}
  % type bibliography here
\end{thebibliography}

\end{document}

%bibliography 
% epub http://idpf.org/epub
% epub201 http://idpf.org/epub/201
% epub301 http://idpf.org/epub/301
% epub2to3 http://www.idpf.org/epub/30/spec/epub30-changes.html
% dangling_else https://en.wikipedia.org/wiki/Dangling_else 
% robots http://www.robotstxt.org/robotstxt.html
% py_mechanize % https://pypi.python.org/pypi/mechanize/
% java_htmlunit http://htmlunit.sourceforge.net/
% worm https://parahumans.wordpress.com/
% flask http://flask.pocoo.org/ 
% bootstrap
% bootstrap_cyborg https://bootswatch.com/cyborg/
% wtforms https://wtforms.readthedocs.io/en/latest/
